{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59b1b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import itertools\n",
    "from sklearn.metrics import  confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5d843a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_cls:\n",
    "    def __init__(self, train_test, **kwargs):\n",
    "        col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "                     \"dst_bytes\", \"land_f\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
    "                     \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
    "                     \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "                     \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
    "                     \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
    "                     \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "                     \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "                     \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "                     \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"labels\", \"dificulty\"]\n",
    "        self.index = 0\n",
    "        # Data formated path and test path.\n",
    "        self.loaded = False\n",
    "        self.train_test = train_test\n",
    "        self.train_path = kwargs.get('train_path', 'datasets/NSL/KDDTrain+.txt')\n",
    "        self.test_path = kwargs.get('test_path',\n",
    "                                    'datasets/NSL/KDDTest+.csv')\n",
    "\n",
    "        self.formated_train_path = kwargs.get('formated_train_path',\n",
    "                                              \"formated_train_adv.data\")\n",
    "        self.formated_test_path = kwargs.get('formated_test_path',\n",
    "                                             \"formated_test_adv.data\")\n",
    "        self.attack_types = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
    "        self.attack_names = []\n",
    "        self.attack_map = {'normal': 'normal',\n",
    "\n",
    "                           'back': 'DoS',\n",
    "                           'land': 'DoS',\n",
    "                           'neptune': 'DoS',\n",
    "                           'pod': 'DoS',\n",
    "                           'smurf': 'DoS',\n",
    "                           'teardrop': 'DoS',\n",
    "                           'mailbomb': 'DoS',\n",
    "                           'apache2': 'DoS',\n",
    "                           'processtable': 'DoS',\n",
    "                           'udpstorm': 'DoS',\n",
    "\n",
    "                           'ipsweep': 'Probe',\n",
    "                           'nmap': 'Probe',\n",
    "                           'portsweep': 'Probe',\n",
    "                           'satan': 'Probe',\n",
    "                           'mscan': 'Probe',\n",
    "                           'saint': 'Probe',\n",
    "\n",
    "                           'ftp_write': 'R2L',\n",
    "                           'guess_passwd': 'R2L',\n",
    "                           'imap': 'R2L',\n",
    "                           'multihop': 'R2L',\n",
    "                           'phf': 'R2L',\n",
    "                           'spy': 'R2L',\n",
    "                           'warezclient': 'R2L',\n",
    "                           'warezmaster': 'R2L',\n",
    "                           'sendmail': 'R2L',\n",
    "                           'named': 'R2L',\n",
    "                           'snmpgetattack': 'R2L',\n",
    "                           'snmpguess': 'R2L',\n",
    "                           'xlock': 'R2L',\n",
    "                           'xsnoop': 'R2L',\n",
    "                           'worm': 'R2L',\n",
    "\n",
    "                           'buffer_overflow': 'U2R',\n",
    "                           'loadmodule': 'U2R',\n",
    "                           'perl': 'U2R',\n",
    "                           'rootkit': 'U2R',\n",
    "                           'httptunnel': 'U2R',\n",
    "                           'ps': 'U2R',\n",
    "                           'sqlattack': 'U2R',\n",
    "                           'xterm': 'U2R'\n",
    "                           }\n",
    "        self.all_attack_names = list(self.attack_map.keys())\n",
    "\n",
    "        formated = False\n",
    "        \n",
    "        # Test formated data exists\n",
    "        if os.path.exists(self.formated_train_path) and os.path.exists(self.formated_test_path):\n",
    "            formated = True\n",
    "\n",
    "        self.formated_dir = \"../datasets/formated/\"\n",
    "        if not os.path.exists(self.formated_dir):\n",
    "            os.makedirs(self.formated_dir)\n",
    "\n",
    "        # If it does not exist, it's needed to format the data\n",
    "        if not formated:\n",
    "            ''' Formating the dataset for ready-2-use data'''\n",
    "            self.df = pd.read_csv(self.train_path, sep=',', names=col_names, index_col=False)\n",
    "            if 'dificulty' in self.df.columns:\n",
    "                self.df.drop('dificulty', axis=1, inplace=True)  # in case of difficulty\n",
    "\n",
    "            data2 = pd.read_csv(self.test_path, sep=',', names=col_names, index_col=False)\n",
    "            if 'dificulty' in data2:\n",
    "                del (data2['dificulty'])\n",
    "            train_indx = self.df.shape[0]\n",
    "            frames = [self.df, data2]\n",
    "            self.df = pd.concat(frames)\n",
    "\n",
    "            # Dataframe processing\n",
    "            self.df = pd.concat([self.df.drop('protocol_type', axis=1), pd.get_dummies(self.df['protocol_type'])],\n",
    "                                axis=1)\n",
    "            self.df = pd.concat([self.df.drop('service', axis=1), pd.get_dummies(self.df['service'])], axis=1)\n",
    "            self.df = pd.concat([self.df.drop('flag', axis=1), pd.get_dummies(self.df['flag'])], axis=1)\n",
    "\n",
    "            # 1 if ``su root'' command attempted; 0 otherwise\n",
    "            self.df['su_attempted'] = self.df['su_attempted'].replace(2.0, 0.0)\n",
    "\n",
    "            # One hot encoding for labels\n",
    "            self.df = pd.concat([self.df.drop('labels', axis=1),\n",
    "                                 pd.get_dummies(self.df['labels'])], axis=1)\n",
    "\n",
    "            # Normalization of the df\n",
    "            # normalized_df=(df-df.mean())/df.std()\n",
    "            for indx, dtype in self.df.dtypes.items():\n",
    "                if dtype == 'float64' or dtype == 'int64':\n",
    "                    if self.df[indx].max() == 0 and self.df[indx].min() == 0:\n",
    "                        self.df[indx] = 0\n",
    "                    else:\n",
    "                        self.df[indx] = (self.df[indx] - self.df[indx].min()) / (\n",
    "                                    self.df[indx].max() - self.df[indx].min())\n",
    "\n",
    "            # Save data\n",
    "            test_df = self.df.iloc[train_indx:self.df.shape[0]]\n",
    "            test_df = shuffle(test_df, random_state=np.random.randint(0, 100))\n",
    "            self.df = self.df[:train_indx]\n",
    "            self.df = shuffle(self.df, random_state=np.random.randint(0, 100))\n",
    "            test_df.to_csv(self.formated_test_path, sep=',', index=False)\n",
    "            self.df.to_csv(self.formated_train_path, sep=',', index=False)\n",
    "\n",
    "            # Create a list with the existent attacks in the df\n",
    "            for att in self.attack_map:\n",
    "                if att in self.df.columns:\n",
    "                    # Add only if there is exist at least 1\n",
    "                    if np.sum(self.df[att].values) > 1:\n",
    "                        self.attack_names.append(att)\n",
    "\n",
    "    def get_shape(self):\n",
    "        if self.loaded is False:\n",
    "            self._load_df()\n",
    "\n",
    "        self.data_shape = self.df.shape\n",
    "        # stata + labels\n",
    "        return self.data_shape\n",
    "\n",
    "    ''' Get n-rows from loaded data \n",
    "        The dataset must be loaded in RAM\n",
    "    '''\n",
    "\n",
    "    def get_batch(self, batch_size=100):\n",
    "        if self.loaded is False:\n",
    "            self._load_df()\n",
    "\n",
    "        # Read the df rows\n",
    "        indexes = list(range(self.index, self.index + batch_size))\n",
    "        if max(indexes) > self.data_shape[0] - 1:\n",
    "            dif = max(indexes) - self.data_shape[0]\n",
    "            indexes[len(indexes) - dif - 1:len(indexes)] = list(range(dif + 1))\n",
    "            self.index = batch_size - dif\n",
    "            batch = self.df.iloc[indexes]\n",
    "        else:\n",
    "            batch = self.df.iloc[indexes]\n",
    "            self.index += batch_size\n",
    "\n",
    "        labels = batch[self.attack_names]\n",
    "\n",
    "        batch = batch.drop(self.all_attack_names, axis=1)\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def get_full(self):\n",
    "        if self.loaded is False:\n",
    "            self._load_df()\n",
    "\n",
    "        labels = self.df[self.attack_names]\n",
    "\n",
    "        batch = self.df.drop(self.all_attack_names, axis=1)\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def _load_df(self):\n",
    "        if self.train_test == 'train':\n",
    "            self.df = pd.read_csv(self.formated_train_path, sep=',')  # Read again the csv\n",
    "        else:\n",
    "            self.df = pd.read_csv(self.formated_test_path, sep=',')\n",
    "        self.index = np.random.randint(0, self.df.shape[0] - 1, dtype=np.int32)\n",
    "        self.loaded = True\n",
    "        # Create a list with the existent attacks in the df\n",
    "        for att in self.attack_map:\n",
    "            if att in self.df.columns:\n",
    "                # Add only if there is exist at least 1\n",
    "                if np.sum(self.df[att].values) > 1:\n",
    "                    self.attack_names.append(att)\n",
    "        # self.headers = list(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9ce2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    \"\"\"\n",
    "    Q-Network Estimator\n",
    "    Represents the global model for the table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, num_actions, hidden_size=100,\n",
    "                 hidden_layers=1, learning_rate=.2, net_name='qv'):\n",
    "        \"\"\"\n",
    "        Initialize the network with the provided shape\n",
    "        \"\"\"\n",
    "        self.net_name = net_name\n",
    "        self.obs_size = obs_size\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Network arquitecture\n",
    "        self.model = Sequential()\n",
    "        # Add imput layer\n",
    "        self.model.add(Dense(hidden_size, input_shape=(obs_size,),\n",
    "                             activation='relu'))\n",
    "        # Add hidden layers\n",
    "        for layers in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        # Add output layer\n",
    "        if self.net_name == 'actor':\n",
    "            self.model.add(Dense(num_actions, activation='softmax'))\n",
    "        else:\n",
    "            self.model.add(Dense(num_actions))\n",
    "\n",
    "        # optimizer = optimizers.SGD(learning_rate)\n",
    "        # optimizer = optimizers.legacy.Adam(alpha=learning_rate)\n",
    "        optimizer = optimizers.legacy.Adam(0.00025)\n",
    "        # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
    "\n",
    "        # Compilation of the model with optimizer and loss\n",
    "        if self.net_name == 'actor':\n",
    "            self.model.compile(loss=sac_loss, optimizer=optimizer)\n",
    "        else:\n",
    "            self.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer)\n",
    "\n",
    "    def predict(self, state, batch_size=1):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "        \"\"\"\n",
    "        return self.model.predict(state, batch_size=batch_size)\n",
    "\n",
    "    def update(self, states, q):\n",
    "        \"\"\"\n",
    "        Updates the estimator with the targets.\n",
    "\n",
    "        Args:\n",
    "          states: Target states\n",
    "          q: Estimated values\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        loss = self.model.train_on_batch(states, q)\n",
    "        return loss\n",
    "\n",
    "    def copy_model(model):\n",
    "        \"\"\"Returns a copy of a keras model.\"\"\"\n",
    "        model.save('tmp_model')\n",
    "        return tf.keras.models.load_model('tmp_model')\n",
    "\n",
    "def sac_loss(y_true, y_pred):\n",
    "    \"\"\" y_true 是 Q(*, action_n), y_pred 是 pi(*, action_n) \"\"\"\n",
    "    qs = 0.2 * tf.math.xlogy(y_pred, y_pred) - y_pred * y_true\n",
    "    return tf.reduce_sum(qs, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements basic replay memory\"\"\"\n",
    "\n",
    "    def __init__(self, observation_size, max_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.num_observed = 0\n",
    "        self.max_size = max_size\n",
    "        self.samples = {\n",
    "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
    "                                       dtype=np.float32).reshape(self.max_size,self.observation_size),\n",
    "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
    "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "               }\n",
    "\n",
    "    def observe(self, state, action, reward, done):\n",
    "        index = self.num_observed % self.max_size\n",
    "        self.samples['obs'][index, :] = state\n",
    "        self.samples['action'][index, :] = action\n",
    "        self.samples['reward'][index, :] = reward\n",
    "        self.samples['terminal'][index, :] = done\n",
    "\n",
    "        self.num_observed += 1\n",
    "\n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        max_index = min(self.num_observed, self.max_size) - 1\n",
    "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
    "\n",
    "        s      = np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32)\n",
    "        s_next = np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32)\n",
    "\n",
    "        a      = self.samples['action'][sampled_indices].reshape(minibatch_size)\n",
    "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
    "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
    "\n",
    "        return (s, a, r, s_next, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7229e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reinforcement learning Agent definition\n",
    "'''\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        self.actions = actions\n",
    "        self.ddqn_time = 100\n",
    "        self.ddqn_update = self.ddqn_time\n",
    "        self.num_actions = len(actions)\n",
    "        self.obs_size = obs_size\n",
    "        self.alpha = 0.2\n",
    "        self.net_learning_rate = 0.1\n",
    "        self.epsilon = kwargs.get('epsilon', 1)\n",
    "        self.min_epsilon = kwargs.get('min_epsilon', .1)\n",
    "        self.gamma = kwargs.get('gamma', .001)\n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 2)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        self.ExpRep = kwargs.get('ExpRep', True)\n",
    "        if self.ExpRep:\n",
    "            self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10))\n",
    "\n",
    "        self.actor_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                      kwargs.get('hidden_size', 100),\n",
    "                                      kwargs.get('hidden_layers', 1),\n",
    "                                      kwargs.get('learning_rate', .2),\n",
    "                                      net_name='actor')\n",
    "        self.q0_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                      kwargs.get('hidden_size', 100),\n",
    "                                      kwargs.get('hidden_layers', 1),\n",
    "                                      kwargs.get('learning_rate', .2))\n",
    "        self.q1_network = QNetwork(self.obs_size, self.num_actions,\n",
    "                                             kwargs.get('hidden_size', 100),\n",
    "                                             kwargs.get('hidden_layers', 1),\n",
    "                                             kwargs.get('learning_rate', .2))\n",
    "        self.v_network = QNetwork(self.obs_size, 1,\n",
    "                                             kwargs.get('hidden_size', 100),\n",
    "                                             kwargs.get('hidden_layers', 1),\n",
    "                                             kwargs.get('learning_rate', .2))\n",
    "        self.target_v_network = QNetwork(self.obs_size, 1,\n",
    "                                  kwargs.get('hidden_size', 100),\n",
    "                                  kwargs.get('hidden_layers', 1),\n",
    "                                  kwargs.get('learning_rate', .2))\n",
    "\n",
    "        self.update_target_net(self.target_v_network.model,\n",
    "                               self.v_network.model, self.net_learning_rate)\n",
    "\n",
    "        print(\"Actor:\", get_flops(self.actor_network.model,batch_size=1))\n",
    "        self.actor_network.model.summary()\n",
    "        print(\"q:\", get_flops(self.q0_network.model, batch_size=1))\n",
    "        self.q0_network.model.summary()\n",
    "        print(\"v:\", get_flops(self.target_v_network.model, batch_size=1))\n",
    "        self.target_v_network.model.summary()\n",
    "\n",
    "    def learn(self, states, actions, next_states, rewards, done):\n",
    "        if self.ExpRep:\n",
    "            self.memory.observe(states, actions, rewards, done)\n",
    "        else:\n",
    "            self.states = states\n",
    "            self.actions = actions\n",
    "            self.next_states = next_states\n",
    "            self.rewards = rewards\n",
    "            self.done = done\n",
    "\n",
    "    def update_model(self):\n",
    "        if self.ExpRep:\n",
    "            (states, actions, rewards, next_states, done) = self.memory.sample_minibatch(self.minibatch_size)\n",
    "        else:\n",
    "            states = self.states\n",
    "            rewards = self.rewards\n",
    "            next_states = self.next_states\n",
    "            actions = self.actions\n",
    "            done = self.done\n",
    "\n",
    "        pis = self.actor_network.predict(states)\n",
    "        q0s = self.q0_network.predict(states)\n",
    "        q1s = self.q1_network.predict(states)\n",
    "\n",
    "        # 训练执行者\n",
    "        loss = self.actor_network.model.train_on_batch(states, q0s)\n",
    "\n",
    "        # 训练评论者\n",
    "        q01s = np.minimum(q0s, q1s)\n",
    "        entropic_q01s = pis * q01s - self.alpha * tf.math.xlogy(pis, pis)\n",
    "        v_targets = tf.math.reduce_sum(entropic_q01s, axis=-1)\n",
    "        self.v_network.model.fit(states, v_targets, verbose=0)\n",
    "\n",
    "        next_vs = self.target_v_network.predict(next_states)\n",
    "        q_targets = rewards.reshape(q0s[range(self.minibatch_size), actions].shape) + self.gamma * (1. - done.reshape(q0s[range(self.minibatch_size), actions].shape)) * \\\n",
    "                    next_vs[:, 0]\n",
    "        q0s[range(self.minibatch_size), actions] = q_targets\n",
    "        q1s[range(self.minibatch_size), actions] = q_targets\n",
    "        self.q0_network.model.fit(states, q0s, verbose=0)\n",
    "        self.q1_network.model.fit(states, q1s, verbose=0)\n",
    "\n",
    "        # 更新目标网络\n",
    "        # self.ddqn_update -= 1\n",
    "        # if self.ddqn_update == 0:\n",
    "        #     self.ddqn_update = self.ddqn_time\n",
    "        #     #            self.target_model_network.model = QNetwork.copy_model(self.model_network.model)\n",
    "        self.update_target_net(self.target_v_network.model,\n",
    "                               self.v_network.model, self.net_learning_rate)\n",
    "        return loss\n",
    "\n",
    "    def act(self, state, policy):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_target_net(self, target_net, evaluate_net, learning_rate=1.):\n",
    "        target_weights = target_net.get_weights()\n",
    "        evaluate_weights = evaluate_net.get_weights()\n",
    "        average_weights = [(1. - learning_rate) * t + learning_rate * e\n",
    "                for t, e in zip(target_weights, evaluate_weights)]\n",
    "        target_net.set_weights(average_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f14e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderAgent(Agent):\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        super().__init__(actions, obs_size, **kwargs)\n",
    "\n",
    "    def act(self, states):\n",
    "        # Get actions under the policy\n",
    "        probs = self.actor_network.model.predict(states)[0]\n",
    "        actions = np.random.choice(self.actions, size=1, p=probs)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33ac650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackAgent(Agent):\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        super().__init__(actions, obs_size, **kwargs)\n",
    "\n",
    "    def act(self, states):\n",
    "        # Get actions under the policy\n",
    "        probs = self.actor_network.model.predict(states)[0]\n",
    "        actions = np.random.choice(self.actions, size=1, p=probs)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "724b2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLenv(data_cls):\n",
    "    def __init__(self, train_test, **kwargs):\n",
    "        data_cls.__init__(self, train_test, **kwargs)\n",
    "        data_cls._load_df(self)\n",
    "        self.data_shape = data_cls.get_shape(self)\n",
    "        self.batch_size = kwargs.get('batch_size', 1)  # experience replay -> batch = 1\n",
    "        self.iterations_episode = kwargs.get('iterations_episode', 10)\n",
    "        if self.batch_size == 'full':\n",
    "            self.batch_size = int(self.data_shape[0] / iterations_episode)\n",
    "\n",
    "    '''\n",
    "    _update_state: function to update the current state\n",
    "    Returns:\n",
    "        None\n",
    "    Modifies the self parameters involved in the state:\n",
    "        self.state and self.labels\n",
    "    Also modifies the true labels to get learning knowledge\n",
    "    '''\n",
    "\n",
    "    def _update_state(self):\n",
    "        self.states, self.labels = data_cls.get_batch(self)\n",
    "\n",
    "        # Update statistics\n",
    "        self.true_labels += np.sum(self.labels).values\n",
    "\n",
    "    '''\n",
    "    Returns:\n",
    "        + Observation of the enviroment\n",
    "    '''\n",
    "\n",
    "    def reset(self):\n",
    "        # Statistics\n",
    "        self.def_true_labels = np.zeros(len(self.attack_types), dtype=int)\n",
    "        self.def_estimated_labels = np.zeros(len(self.attack_types), dtype=int)\n",
    "        self.att_true_labels = np.zeros(len(self.attack_names), dtype=int)\n",
    "\n",
    "        self.state_numb = 0\n",
    "\n",
    "        data_cls._load_df(self)  # Reload and random index\n",
    "        self.states, self.labels = data_cls.get_batch(self, self.batch_size)\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.steps_in_episode = 0\n",
    "        return self.states.values\n",
    "\n",
    "    '''\n",
    "    Returns:\n",
    "        State: Next state for the game\n",
    "        Reward: Actual reward\n",
    "        done: If the game ends (no end in this case)\n",
    "\n",
    "    In the adversarial enviroment, it's only needed to return the actual reward\n",
    "    '''\n",
    "\n",
    "    def act(self, defender_actions, attack_actions):\n",
    "        # Clear previous rewards\n",
    "        self.att_reward = np.zeros(len(attack_actions))\n",
    "        self.def_reward = np.zeros(len(defender_actions))\n",
    "\n",
    "        attack = [self.attack_types.index(self.attack_map[self.attack_names[att]]) for att in attack_actions]\n",
    "        # 设置不同的奖励值\n",
    "        if attack[0] in [3,4]:\n",
    "            self.def_reward = (np.asarray(defender_actions) == np.asarray(attack)) * 2\n",
    "            self.att_reward = (np.asarray(defender_actions) != np.asarray(attack)) * 2\n",
    "        # # elif attack[0] in [3]:\n",
    "        # #     self.def_reward = (np.asarray(defender_actions) == np.asarray(attack)) * 2\n",
    "        # #     self.att_reward = (np.asarray(defender_actions) != np.asarray(attack)) * 2\n",
    "        else:\n",
    "            self.def_reward = (np.asarray(defender_actions) == np.asarray(attack)) * 1\n",
    "            self.att_reward = (np.asarray(defender_actions) != np.asarray(attack)) * 1\n",
    "\n",
    "        self.def_estimated_labels += np.bincount(defender_actions, minlength=len(self.attack_types))\n",
    "        # TODO\n",
    "        # list comprehension\n",
    "\n",
    "        for act in attack_actions:\n",
    "            self.def_true_labels[self.attack_types.index(self.attack_map[self.attack_names[act]])] += 1\n",
    "\n",
    "        # Get new state and new true values\n",
    "        attack_actions = attacker_agent.act(self.states)\n",
    "        self.states = env.get_states(attack_actions)\n",
    "\n",
    "        # Done allways false in this continuous task\n",
    "        self.done = np.zeros(len(attack_actions), dtype=bool)\n",
    "\n",
    "        return self.states, self.def_reward, self.att_reward, attack_actions, self.done\n",
    "\n",
    "    '''\n",
    "    Provide the actual states for the selected attacker actions\n",
    "    Parameters:\n",
    "        self:\n",
    "        attacker_actions: optimum attacks selected by the attacker\n",
    "            it can be one of attack_names list and select random of this\n",
    "    Returns:\n",
    "        State: Actual state for the selected attacks\n",
    "    '''\n",
    "\n",
    "    def get_states(self, attacker_actions):\n",
    "        first = True\n",
    "        for attack in attacker_actions:\n",
    "            if first:\n",
    "                minibatch = (self.df[self.df[self.attack_names[attack]] == 1].sample(1))\n",
    "                first = False\n",
    "            else:\n",
    "                minibatch = minibatch.append(self.df[self.df[self.attack_names[attack]] == 1].sample(1))\n",
    "\n",
    "        self.labels = minibatch[self.attack_names]\n",
    "        minibatch.drop(self.all_attack_names, axis=1, inplace=True)\n",
    "        self.states = minibatch\n",
    "\n",
    "        return self.states\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bbe0363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/2.12m flops)\n",
      "  sequential_95/dense_381/MatMul (980.00k/980.00k flops)\n",
      "  sequential_95/dense_382/MatMul (980.00k/980.00k flops)\n",
      "  sequential_95/dense_380/MatMul (147.00k/147.00k flops)\n",
      "  sequential_95/dense_383/MatMul (7.00k/7.00k flops)\n",
      "  sequential_95/dense_380/BiasAdd (700/700 flops)\n",
      "  sequential_95/dense_381/BiasAdd (700/700 flops)\n",
      "  sequential_95/dense_382/BiasAdd (700/700 flops)\n",
      "  sequential_95/dense_383/Softmax (25/25 flops)\n",
      "  sequential_95/dense_383/BiasAdd (5/5 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "Actor: 2116130\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_380 (Dense)           (None, 700)               74200     \n",
      "                                                                 \n",
      " dense_381 (Dense)           (None, 700)               490700    \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 700)               490700    \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 5)                 3505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,059,105\n",
      "Trainable params: 1,059,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/2.12m flops)\n",
      "  sequential_96/dense_385/MatMul (980.00k/980.00k flops)\n",
      "  sequential_96/dense_386/MatMul (980.00k/980.00k flops)\n",
      "  sequential_96/dense_384/MatMul (147.00k/147.00k flops)\n",
      "  sequential_96/dense_387/MatMul (7.00k/7.00k flops)\n",
      "  sequential_96/dense_384/BiasAdd (700/700 flops)\n",
      "  sequential_96/dense_385/BiasAdd (700/700 flops)\n",
      "  sequential_96/dense_386/BiasAdd (700/700 flops)\n",
      "  sequential_96/dense_387/BiasAdd (5/5 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "q: 2116105\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_384 (Dense)           (None, 700)               74200     \n",
      "                                                                 \n",
      " dense_385 (Dense)           (None, 700)               490700    \n",
      "                                                                 \n",
      " dense_386 (Dense)           (None, 700)               490700    \n",
      "                                                                 \n",
      " dense_387 (Dense)           (None, 5)                 3505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,059,105\n",
      "Trainable params: 1,059,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/2.11m flops)\n",
      "  sequential_99/dense_397/MatMul (980.00k/980.00k flops)\n",
      "  sequential_99/dense_398/MatMul (980.00k/980.00k flops)\n",
      "  sequential_99/dense_396/MatMul (147.00k/147.00k flops)\n",
      "  sequential_99/dense_399/MatMul (1.40k/1.40k flops)\n",
      "  sequential_99/dense_396/BiasAdd (700/700 flops)\n",
      "  sequential_99/dense_397/BiasAdd (700/700 flops)\n",
      "  sequential_99/dense_398/BiasAdd (700/700 flops)\n",
      "  sequential_99/dense_399/BiasAdd (1/1 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "v: 2110501\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_396 (Dense)           (None, 700)               74200     \n",
      "                                                                 \n",
      " dense_397 (Dense)           (None, 700)               490700    \n",
      "                                                                 \n",
      " dense_398 (Dense)           (None, 700)               490700    \n",
      "                                                                 \n",
      " dense_399 (Dense)           (None, 1)                 701       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,056,301\n",
      "Trainable params: 1,056,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 13:04:42.474321: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-02-12 13:04:42.474393: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2024-02-12 13:04:42.516459: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-02-12 13:04:42.516513: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2024-02-12 13:04:42.573605: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-02-12 13:04:42.573675: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/66.04k flops)\n",
      "  sequential_100/dense_400/MatMul (21.00k/21.00k flops)\n",
      "  sequential_100/dense_401/MatMul (20.00k/20.00k flops)\n",
      "  sequential_100/dense_402/MatMul (20.00k/20.00k flops)\n",
      "  sequential_100/dense_403/MatMul (4.60k/4.60k flops)\n",
      "  sequential_100/dense_403/Softmax (115/115 flops)\n",
      "  sequential_100/dense_400/BiasAdd (100/100 flops)\n",
      "  sequential_100/dense_401/BiasAdd (100/100 flops)\n",
      "  sequential_100/dense_402/BiasAdd (100/100 flops)\n",
      "  sequential_100/dense_403/BiasAdd (23/23 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "Actor: 66038\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_400 (Dense)           (None, 100)               10600     \n",
      "                                                                 \n",
      " dense_401 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_402 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_403 (Dense)           (None, 23)                2323      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,123\n",
      "Trainable params: 33,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/65.92k flops)\n",
      "  sequential_101/dense_404/MatMul (21.00k/21.00k flops)\n",
      "  sequential_101/dense_405/MatMul (20.00k/20.00k flops)\n",
      "  sequential_101/dense_406/MatMul (20.00k/20.00k flops)\n",
      "  sequential_101/dense_407/MatMul (4.60k/4.60k flops)\n",
      "  sequential_101/dense_404/BiasAdd (100/100 flops)\n",
      "  sequential_101/dense_405/BiasAdd (100/100 flops)\n",
      "  sequential_101/dense_406/BiasAdd (100/100 flops)\n",
      "  sequential_101/dense_407/BiasAdd (23/23 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "q: 65923\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_404 (Dense)           (None, 100)               10600     \n",
      "                                                                 \n",
      " dense_405 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_406 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_407 (Dense)           (None, 23)                2323      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,123\n",
      "Trainable params: 33,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/61.50k flops)\n",
      "  sequential_104/dense_416/MatMul (21.00k/21.00k flops)\n",
      "  sequential_104/dense_417/MatMul (20.00k/20.00k flops)\n",
      "  sequential_104/dense_418/MatMul (20.00k/20.00k flops)\n",
      "  sequential_104/dense_419/MatMul (200/200 flops)\n",
      "  sequential_104/dense_416/BiasAdd (100/100 flops)\n",
      "  sequential_104/dense_417/BiasAdd (100/100 flops)\n",
      "  sequential_104/dense_418/BiasAdd (100/100 flops)\n",
      "  sequential_104/dense_419/BiasAdd (1/1 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "v: 61501\n",
      "Model: \"sequential_104\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_416 (Dense)           (None, 100)               10600     \n",
      "                                                                 \n",
      " dense_417 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_418 (Dense)           (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_419 (Dense)           (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,901\n",
      "Trainable params: 30,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 13:04:42.706022: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-02-12 13:04:42.706087: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2024-02-12 13:04:42.743581: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-02-12 13:04:42.743651: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2024-02-12 13:04:42.777649: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-02-12 13:04:42.777704: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n"
     ]
    }
   ],
   "source": [
    "kdd_train = \"/Users/bpratyush/Downloads/Project/Dataset/KDDTrain+.csv\"\n",
    "kdd_test = \"/Users/bpratyush/Downloads/Project/Dataset/KDDTest+.csv\"\n",
    "\n",
    "formated_train_path = \"formated_train_adv.data\"\n",
    "formated_test_path = \"formated_test_adv.data\"\n",
    "\n",
    "# Train batch\n",
    "batch_size = 1\n",
    "# batch of memory ExpRep\n",
    "minibatch_size = 180\n",
    "ExpRep = True\n",
    "\n",
    "iterations_episode = 180\n",
    "\n",
    "# Initialization of the enviroment\n",
    "env = RLenv('train', train_path=kdd_train, test_path=kdd_test,\n",
    "            formated_train_path=formated_train_path,\n",
    "            formated_test_path=formated_test_path, batch_size=batch_size,\n",
    "            iterations_episode=iterations_episode)\n",
    "# obs_size = size of the state\n",
    "obs_size = env.data_shape[1] - len(env.all_attack_names)\n",
    "\n",
    "# num_episodes = int(env.data_shape[0]/(iterations_episode)/10)\n",
    "num_episodes = 100\n",
    "\n",
    "'''\n",
    "Definition for the defensor agent.\n",
    "'''\n",
    "defender_valid_actions = list(range(len(env.attack_types)))  # only detect type of attack\n",
    "defender_num_actions = len(defender_valid_actions)\n",
    "\n",
    "def_epsilon = 1  # exploration\n",
    "min_epsilon = 0.01  # min value for exploration\n",
    "def_gamma = 0.001\n",
    "def_decay_rate = 0.99\n",
    "\n",
    "def_hidden_size = 700\n",
    "def_hidden_layers = 2\n",
    "\n",
    "def_learning_rate = .2\n",
    "\n",
    "defender_agent = DefenderAgent(defender_valid_actions, obs_size,\n",
    "                               epoch_length=iterations_episode,\n",
    "                               epsilon=def_epsilon,\n",
    "                               min_epsilon=min_epsilon,\n",
    "                               decay_rate=def_decay_rate,\n",
    "                               gamma=def_gamma,\n",
    "                               hidden_size=def_hidden_size,\n",
    "                               hidden_layers=def_hidden_layers,\n",
    "                               minibatch_size=minibatch_size,\n",
    "                               mem_size=1000,\n",
    "                               learning_rate=def_learning_rate,\n",
    "                               ExpRep=ExpRep)\n",
    "# Pretrained defender\n",
    "# defender_agent.model_network.model.load_weights(\"models/type_model.h5\")\n",
    "\n",
    "'''\n",
    "Definition for the attacker agent.\n",
    "In this case the exploration is better to be greater\n",
    "The correlation sould be greater too so gamma bigger\n",
    "'''\n",
    "attack_valid_actions = list(range(len(env.attack_names)))\n",
    "attack_num_actions = len(attack_valid_actions)\n",
    "\n",
    "att_epsilon = 1\n",
    "min_epsilon = 0.82  # min value for exploration\n",
    "\n",
    "att_gamma = 0.001\n",
    "att_decay_rate = 0.99\n",
    "\n",
    "att_hidden_layers = 2\n",
    "att_hidden_size = 100\n",
    "\n",
    "att_learning_rate = 0.2\n",
    "\n",
    "attacker_agent = AttackAgent(attack_valid_actions, obs_size,\n",
    "                             epoch_length=iterations_episode,\n",
    "                             epsilon=att_epsilon,\n",
    "                             min_epsilon=min_epsilon,\n",
    "                             decay_rate=att_decay_rate,\n",
    "                             gamma=att_gamma,\n",
    "                             hidden_size=att_hidden_size,\n",
    "                             hidden_layers=att_hidden_layers,\n",
    "                             minibatch_size=minibatch_size,\n",
    "                             mem_size=1000,\n",
    "                             learning_rate=att_learning_rate,\n",
    "                             ExpRep=ExpRep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48337fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Total epoch: 100 | Iterations in epoch: 180| Minibatch from mem size: 180 | Total Samples: 18000|\n",
      "-------------------------------------------------------------------------------\n",
      "Dataset shape: (125973, 145)\n",
      "-------------------------------------------------------------------------------\n",
      "Attacker parameters: Num_actions=23 | gamma=0.001 | epsilon=1 | ANN hidden size=100 | ANN hidden layers=2|\n",
      "-------------------------------------------------------------------------------\n",
      "Defense parameters: Num_actions=5 | gamma=0.001 | epsilon=1 | ANN hidden size=700 | ANN hidden layers=2|\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['mailbomb', 'apache2', 'processtable', 'udpstorm', 'mscan', 'saint', 'sendmail', 'named', 'snmpgetattack', 'snmpguess', 'xlock', 'xsnoop', 'worm', 'httptunnel', 'ps', 'sqlattack', 'xterm'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m att_total_reward_by_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Reset enviromet, actualize the data batch with random state/attacks\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m states \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Get actions for actual states following the policy\u001b[39;00m\n\u001b[1;32m     44\u001b[0m attack_actions \u001b[38;5;241m=\u001b[39m attacker_agent\u001b[38;5;241m.\u001b[39mact(states)\n",
      "Cell \u001b[0;32mIn[42], line 40\u001b[0m, in \u001b[0;36mRLenv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_numb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     39\u001b[0m data_cls\u001b[38;5;241m.\u001b[39m_load_df(\u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# Reload and random index\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m data_cls\u001b[38;5;241m.\u001b[39mget_batch(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_in_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[26], line 165\u001b[0m, in \u001b[0;36mdata_cls.get_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    163\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_names]\n\u001b[0;32m--> 165\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_attack_names, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch, labels\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5259\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5260\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5261\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5262\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5263\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5264\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5265\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5266\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['mailbomb', 'apache2', 'processtable', 'udpstorm', 'mscan', 'saint', 'sendmail', 'named', 'snmpgetattack', 'snmpguess', 'xlock', 'xsnoop', 'worm', 'httptunnel', 'ps', 'sqlattack', 'xterm'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "att_reward_chain = []\n",
    "def_reward_chain = []\n",
    "att_loss_chain = []\n",
    "def_loss_chain = []\n",
    "def_total_reward_chain = []\n",
    "att_total_reward_chain = []\n",
    "\n",
    "# Print parameters\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(\"Total epoch: {} | Iterations in epoch: {}\"\n",
    "      \"| Minibatch from mem size: {} | Total Samples: {}|\".format(num_episodes,\n",
    "                                                                  iterations_episode, minibatch_size,\n",
    "                                                                  num_episodes * iterations_episode))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(\"Dataset shape: {}\".format(env.data_shape))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(\"Attacker parameters: Num_actions={} | gamma={} |\"\n",
    "      \" epsilon={} | ANN hidden size={} | \"\n",
    "      \"ANN hidden layers={}|\".format(attack_num_actions,\n",
    "                                     att_gamma, att_epsilon, att_hidden_size,\n",
    "                                     att_hidden_layers))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "print(\"Defense parameters: Num_actions={} | gamma={} | \"\n",
    "      \"epsilon={} | ANN hidden size={} |\"\n",
    "      \" ANN hidden layers={}|\".format(defender_num_actions,\n",
    "                                      def_gamma, def_epsilon, def_hidden_size,\n",
    "                                      def_hidden_layers))\n",
    "print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "# Main loop\n",
    "attacks_by_epoch = []\n",
    "attack_labels_list = []\n",
    "for epoch in range(num_episodes):\n",
    "    start_time = time.time()\n",
    "    att_loss = 0.\n",
    "    def_loss = 0.\n",
    "    def_total_reward_by_episode = 0\n",
    "    att_total_reward_by_episode = 0\n",
    "    # Reset enviromet, actualize the data batch with random state/attacks\n",
    "    states = env.reset()\n",
    "\n",
    "    # Get actions for actual states following the policy\n",
    "    attack_actions = attacker_agent.act(states)\n",
    "    states = env.get_states(attack_actions)\n",
    "\n",
    "    done = False\n",
    "\n",
    "    attacks_list = []\n",
    "    # Iteration in one episode\n",
    "    for i_iteration in range(iterations_episode):\n",
    "\n",
    "        attacks_list.append(attack_actions[0])\n",
    "        # apply actions, get rewards and new state\n",
    "        act_time = time.time()\n",
    "        defender_actions = defender_agent.act(states)\n",
    "        # Enviroment actuation for this actions\n",
    "        next_states, def_reward, att_reward, next_attack_actions, done = env.act(defender_actions, attack_actions)\n",
    "        # If the epoch*batch_size*iterations_episode is largest than the df\n",
    "\n",
    "        attacker_agent.learn(states, attack_actions, next_states, att_reward, done)\n",
    "        defender_agent.learn(states, defender_actions, next_states, def_reward, done)\n",
    "\n",
    "        act_end_time = time.time()\n",
    "\n",
    "        # Train network, update loss after at least minibatch_learns\n",
    "        if ExpRep and epoch * iterations_episode + i_iteration >= minibatch_size:\n",
    "            def_loss += defender_agent.update_model()\n",
    "            att_loss += attacker_agent.update_model()\n",
    "        elif not ExpRep:\n",
    "            def_loss += defender_agent.update_model()\n",
    "            att_loss += attacker_agent.update_model()\n",
    "\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        # Update the state\n",
    "        states = next_states\n",
    "        attack_actions = next_attack_actions\n",
    "\n",
    "        # Update statistics\n",
    "        def_total_reward_by_episode += np.sum(def_reward, dtype=np.int32)\n",
    "        att_total_reward_by_episode += np.sum(att_reward, dtype=np.int32)\n",
    "\n",
    "    attacks_by_epoch.append(attacks_list)\n",
    "    # Update user view\n",
    "    def_reward_chain.append(def_total_reward_by_episode)\n",
    "    att_reward_chain.append(att_total_reward_by_episode)\n",
    "    def_loss_chain.append(def_loss)\n",
    "    att_loss_chain.append(att_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"\\r\\n|Epoch {:03d}/{:03d}| time: {:2.2f}|\\r\\n\"\n",
    "          \"|Def Loss {:4.4f} | Def Reward in ep {:03d}|\\r\\n\"\n",
    "          \"|Att Loss {:4.4f} | Att Reward in ep {:03d}|\"\n",
    "          .format(epoch, num_episodes, (end_time - start_time),\n",
    "                  def_loss, def_total_reward_by_episode,\n",
    "                  att_loss, att_total_reward_by_episode))\n",
    "\n",
    "    print(\"|Def Estimated: {}| Att Labels: {}\".format(env.def_estimated_labels,\n",
    "                                                      env.def_true_labels))\n",
    "    attack_labels_list.append(env.def_true_labels)\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc083957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model weights and architecture, used in test\n",
    "defender_agent.actor_network.model.save_weights(\"models/defender_agent_model_0_1.h5\", overwrite=True)\n",
    "with open(\"models/defender_agent_model_0_1.json\", \"w\") as outfile:\n",
    "    json.dump(defender_agent.actor_network.model.to_json(), outfile)\n",
    "\n",
    "formated_test_path = \"formated_test_adv.data\"\n",
    "\n",
    "with open(\"models/defender_agent_model.json\", \"r\") as jfile:\n",
    "    model = model_from_json(json.load(jfile))\n",
    "model.load_weights(\"models/opt-8415/defender_agent_model.h5\")\n",
    "\n",
    "model.compile(loss=sac_loss, optimizer=\"sgd\")\n",
    "\n",
    "# Define environment, game, make sure the batch_size is the same in train\n",
    "env_test = RLenv('test', formated_test_path=formated_test_path)\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "true_labels = np.zeros(len(env_test.attack_types), dtype=int)\n",
    "estimated_labels = np.zeros(len(env_test.attack_types), dtype=int)\n",
    "estimated_correct_labels = np.zeros(len(env_test.attack_types), dtype=int)\n",
    "\n",
    "# states , labels = env.get_sequential_batch(test_path,batch_size = env.batch_size)\n",
    "states, labels = env_test.get_full()\n",
    "\n",
    "start_time = time.time()\n",
    "q = model.predict(states)\n",
    "actions = np.argmax(q, axis=1)\n",
    "end_time = time.time()\n",
    "print(\"predit time:\", end_time -start_time)\n",
    "\n",
    "maped = []\n",
    "for indx, label in labels.iterrows():\n",
    "    maped.append(env_test.attack_types.index(env_test.attack_map[label.idxmax()]))\n",
    "\n",
    "labels, counts = np.unique(maped, return_counts=True)\n",
    "true_labels[labels] += counts\n",
    "\n",
    "for indx, a in enumerate(actions):\n",
    "    estimated_labels[a] += 1\n",
    "    if a == maped[indx]:\n",
    "        total_reward += 1\n",
    "        estimated_correct_labels[a] += 1\n",
    "\n",
    "action_dummies = pd.get_dummies(actions)\n",
    "posible_actions = np.arange(len(env_test.attack_types))\n",
    "for non_existing_action in posible_actions:\n",
    "    if non_existing_action not in action_dummies.columns:\n",
    "        action_dummies[non_existing_action] = np.uint8(0)\n",
    "labels_dummies = pd.get_dummies(maped)\n",
    "\n",
    "normal_f1_score = f1_score(labels_dummies[0].values, action_dummies[0].values)\n",
    "dos_f1_score = f1_score(labels_dummies[1].values, action_dummies[1].values)\n",
    "probe_f1_score = f1_score(labels_dummies[2].values, action_dummies[2].values)\n",
    "r2l_f1_score = f1_score(labels_dummies[3].values, action_dummies[3].values)\n",
    "u2r_f1_score = f1_score(labels_dummies[4].values, action_dummies[4].values)\n",
    "\n",
    "Accuracy = [normal_f1_score, dos_f1_score, probe_f1_score, r2l_f1_score, u2r_f1_score]\n",
    "Mismatch = estimated_labels - true_labels\n",
    "\n",
    "acc = float(100 * total_reward / len(states))\n",
    "print('\\r\\nTotal reward: {} | Number of samples: {} | Accuracy = {:.2f}%'.format(total_reward,\n",
    "                                                                                     len(states), acc))\n",
    "outputs_df = pd.DataFrame(index=env_test.attack_types, columns=[\"Estimated\", \"Correct\", \"Total\", \"F1_score\"])\n",
    "for indx, att in enumerate(env_test.attack_types):\n",
    "    outputs_df.iloc[indx].Estimated = estimated_labels[indx]\n",
    "    outputs_df.iloc[indx].Correct = estimated_correct_labels[indx]\n",
    "    outputs_df.iloc[indx].Total = true_labels[indx]\n",
    "    outputs_df.iloc[indx].F1_score = Accuracy[indx] * 100\n",
    "    outputs_df.iloc[indx].Mismatch = abs(Mismatch[indx])\n",
    "\n",
    "print(outputs_df)\n",
    "aggregated_data_test = np.array(maped)\n",
    "print('Performance measures on Test data')\n",
    "print('Accuracy =  {:.4f}'.format(accuracy_score(aggregated_data_test, actions)))\n",
    "print('F1 =  {:.4f}'.format(f1_score(aggregated_data_test, actions, average='weighted')))\n",
    "print('Precision_score =  {:.4f}'.format(precision_score(aggregated_data_test, actions, average='weighted')))\n",
    "print('recall_score =  {:.4f}'.format(recall_score(aggregated_data_test, actions, average='weighted')))\n",
    "\n",
    "cnf_matrix = confusion_matrix(aggregated_data_test, actions)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=env.attack_types, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    # plt.savefig('confusion_matrix_adversarial.svg', format='svg', dpi=1000)\n",
    "plt.savefig('confusion_matrix_adversarial.png',  dpi=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
