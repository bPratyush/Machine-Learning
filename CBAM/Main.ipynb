{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04123b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7570207118988037\n",
      "Epoch [11/100], Loss: 0.7676351070404053\n",
      "Epoch [21/100], Loss: 0.7045150399208069\n",
      "Epoch [31/100], Loss: 0.6798362731933594\n",
      "Epoch [41/100], Loss: 0.7505645155906677\n",
      "Epoch [51/100], Loss: 0.7027023434638977\n",
      "Epoch [61/100], Loss: 0.7361545562744141\n",
      "Epoch [71/100], Loss: 0.6793601512908936\n",
      "Epoch [81/100], Loss: 0.6683942079544067\n",
      "Epoch [91/100], Loss: 0.7636892199516296\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# CustomDataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, num_samples, num_channels, height, width):\n",
    "        self.num_samples = num_samples\n",
    "        self.data = torch.randn(num_samples, num_channels, height, width)\n",
    "        self.labels = torch.randint(0, 2, (num_samples,), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 64\n",
    "dataset = CustomDataset(num_samples=1000, num_channels=64, height=64, width=64)\n",
    "\n",
    "# Data Loader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class ChannelAttentionModule(nn.Module):  \n",
    "    def __init__(self, F, r=16):  \n",
    "        super(ChannelAttentionModule, self).__init__()  \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv2d(F, F // r, 1, bias=False),  \n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(F // r, F, 1, bias=False)) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "class SpatialAttentionModule(nn.Module):  \n",
    "    def __init__(self, spatial_r=7):\n",
    "        super(SpatialAttentionModule, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size=spatial_r, padding=spatial_r//2, bias=False)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAMModule(nn.Module):\n",
    "    def __init__(self, F, r, spatial_r=7):  \n",
    "        super(CBAMModule, self).__init__()\n",
    "        self.channel_attention = ChannelAttentionModule(F, r) \n",
    "        self.spatial_attention = SpatialAttentionModule(spatial_r)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.spatial_attention(self.channel_attention(x))\n",
    "\n",
    "model = CBAMModule(F=64, r=16)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "weight_decay = 0.001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs.squeeze(), batch_labels)\n",
    "\n",
    "        # L2 Regularisation\n",
    "        l2_reg = torch.tensor(0.0)\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, p=2)\n",
    "\n",
    "        # Adding L2 regularisation term to the loss\n",
    "        loss += weight_decay * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db09bb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CBAMModule                               [1, 1, 1, 1]              --\n",
       "├─ChannelAttentionModule: 1-1            [1, 64, 1, 1]             --\n",
       "│    └─AdaptiveAvgPool2d: 2-1            [1, 64, 1, 1]             --\n",
       "│    └─Sequential: 2-2                   [1, 64, 1, 1]             --\n",
       "│    │    └─Conv2d: 3-1                  [1, 4, 1, 1]              256\n",
       "│    │    └─ReLU: 3-2                    [1, 4, 1, 1]              --\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 1, 1]             256\n",
       "│    └─AdaptiveMaxPool2d: 2-3            [1, 64, 1, 1]             --\n",
       "│    └─Sequential: 2-4                   [1, 64, 1, 1]             (recursive)\n",
       "│    │    └─Conv2d: 3-4                  [1, 4, 1, 1]              (recursive)\n",
       "│    │    └─ReLU: 3-5                    [1, 4, 1, 1]              --\n",
       "│    │    └─Conv2d: 3-6                  [1, 64, 1, 1]             (recursive)\n",
       "│    └─Sigmoid: 2-5                      [1, 64, 1, 1]             --\n",
       "├─SpatialAttentionModule: 1-2            [1, 1, 1, 1]              --\n",
       "│    └─Conv2d: 2-6                       [1, 1, 1, 1]              98\n",
       "│    └─Sigmoid: 2-7                      [1, 1, 1, 1]              --\n",
       "==========================================================================================\n",
       "Total params: 610\n",
       "Trainable params: 610\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 1.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=[1, 64, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fbb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
