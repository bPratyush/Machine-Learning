{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941fee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = \".../Users/bpratyush/Downloads/Machine Learning/CBAM/DataImg/imagesTr\"\n",
    "validation_set = \".../Users/bpratyush/Downloads/Machine Learning/CBAM/DataImg/imagesTs\"\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = \".../Users/bpratyush/Downloads/Machine Learning/CBAM/DataImg/labelsTr\"\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb99b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6407, 0.8070, 0.7564, 0.1232, 0.7252, 0.1572, 0.8369, 0.3923, 0.2875,\n",
      "         0.7351],\n",
      "        [0.4472, 0.0684, 0.2439, 0.1841, 0.7917, 0.4915, 0.5985, 0.1241, 0.6125,\n",
      "         0.1739],\n",
      "        [0.0247, 0.7884, 0.7991, 0.4561, 0.9327, 0.2499, 0.7841, 0.1352, 0.4219,\n",
      "         0.4335],\n",
      "        [0.2526, 0.8878, 0.3045, 0.1470, 0.4767, 0.1478, 0.4962, 0.2444, 0.3454,\n",
      "         0.7645]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.2936513423919678\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb3ae3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6070149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf4f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 1.6854410924613477\n",
      "  batch 2000 loss: 0.8153189923334867\n",
      "  batch 3000 loss: 0.6975528459300986\n",
      "  batch 4000 loss: 0.6781775906318799\n",
      "  batch 5000 loss: 0.6110388081390411\n",
      "  batch 6000 loss: 0.5682162830553716\n",
      "  batch 7000 loss: 0.5421449011211517\n",
      "  batch 8000 loss: 0.5152215704313712\n",
      "  batch 9000 loss: 0.49824270855740177\n",
      "  batch 10000 loss: 0.47368073220923546\n",
      "  batch 11000 loss: 0.48439453057735227\n",
      "  batch 12000 loss: 0.4552974298899062\n",
      "  batch 13000 loss: 0.4620499271423323\n",
      "  batch 14000 loss: 0.41406133158050945\n",
      "  batch 15000 loss: 0.41751225175196305\n",
      "LOSS train 0.41751225175196305 valid 0.43503203988075256\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.38955410487827613\n",
      "  batch 2000 loss: 0.404279209953238\n",
      "  batch 3000 loss: 0.4106898660585284\n",
      "  batch 4000 loss: 0.38914611362297\n",
      "  batch 5000 loss: 0.40486345424508907\n",
      "  batch 6000 loss: 0.38155813825895896\n",
      "  batch 7000 loss: 0.3717478927670163\n",
      "  batch 8000 loss: 0.37039034651158\n",
      "  batch 9000 loss: 0.35930028790951474\n",
      "  batch 10000 loss: 0.37052537198102803\n",
      "  batch 11000 loss: 0.3752897182767629\n",
      "  batch 12000 loss: 0.34561771865407354\n",
      "  batch 13000 loss: 0.35734556725271976\n",
      "  batch 14000 loss: 0.3484505011043511\n",
      "  batch 15000 loss: 0.3645845509398496\n",
      "LOSS train 0.3645845509398496 valid 0.36240291595458984\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.3334070584279607\n",
      "  batch 2000 loss: 0.33769500438519756\n",
      "  batch 3000 loss: 0.34888275797531243\n",
      "  batch 4000 loss: 0.34390806737932145\n",
      "  batch 5000 loss: 0.32194637202867304\n",
      "  batch 6000 loss: 0.34786363546398935\n",
      "  batch 7000 loss: 0.31369356969255024\n",
      "  batch 8000 loss: 0.31756838559545575\n",
      "  batch 9000 loss: 0.32323203891006413\n",
      "  batch 10000 loss: 0.30565601838687145\n",
      "  batch 11000 loss: 0.3254379661924031\n",
      "  batch 12000 loss: 0.3120548342417751\n",
      "  batch 13000 loss: 0.32544768793633555\n",
      "  batch 14000 loss: 0.3338275504006597\n",
      "  batch 15000 loss: 0.32973763612595214\n",
      "LOSS train 0.32973763612595214 valid 0.3349514305591583\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.2971120265768259\n",
      "  batch 2000 loss: 0.3085631252678577\n",
      "  batch 3000 loss: 0.30325473634831723\n",
      "  batch 4000 loss: 0.30758986806654504\n",
      "  batch 5000 loss: 0.2969264777226199\n",
      "  batch 6000 loss: 0.3125416093933891\n",
      "  batch 7000 loss: 0.3030461955123319\n",
      "  batch 8000 loss: 0.3096951444190927\n",
      "  batch 9000 loss: 0.2970342758145116\n",
      "  batch 10000 loss: 0.320602320991391\n",
      "  batch 11000 loss: 0.294013492166574\n",
      "  batch 12000 loss: 0.29662357469361084\n",
      "  batch 13000 loss: 0.28089132620228335\n",
      "  batch 14000 loss: 0.30175821878191345\n",
      "  batch 15000 loss: 0.3087706546798945\n",
      "LOSS train 0.3087706546798945 valid 0.32082247734069824\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.26721997587452645\n",
      "  batch 2000 loss: 0.2867889348202789\n",
      "  batch 3000 loss: 0.28448069049006053\n",
      "  batch 4000 loss: 0.28330269209275866\n",
      "  batch 5000 loss: 0.30075027824399875\n",
      "  batch 6000 loss: 0.2821245093401376\n",
      "  batch 7000 loss: 0.29990400493524455\n",
      "  batch 8000 loss: 0.28560554517684067\n",
      "  batch 9000 loss: 0.2722793038631053\n",
      "  batch 10000 loss: 0.27985556890117\n",
      "  batch 11000 loss: 0.29071127820783477\n",
      "  batch 12000 loss: 0.2904632253494328\n",
      "  batch 13000 loss: 0.2663125273042097\n",
      "  batch 14000 loss: 0.2673834548519017\n",
      "  batch 15000 loss: 0.26743621088371217\n",
      "LOSS train 0.26743621088371217 valid 0.32182300090789795\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b5b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
